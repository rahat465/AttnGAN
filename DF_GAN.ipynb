{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DF_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyPySY9d57NQYPJsr5+Ypv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahat465/AttnGAN/blob/master/DF_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFw83wD-C2hf",
        "outputId": "dbc28ca4-cf95-4752-ddde-ce1dc6c74ad9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deleting bird folders"
      ],
      "metadata": {
        "id": "wdUtehYSIW_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/drive/MyDrive/DF-GAN/code/samples/bird/2022_08_11_08_52_44')"
      ],
      "metadata": {
        "id": "q7v7CSdnITWn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DF-GAN/code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKon5eBsEtaa",
        "outputId": "fa71171a-2591-455f-a5c7-ea61cc397641"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DF-GAN/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/sample.sh ./cfg/bird.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N7fpwI0E_fv",
        "outputId": "92f1b40c-d580-4913-9b67-59d790efded2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================>>-----In the---- sample---->>---main---- function\n",
            "==========================>>-----In the ----sample--->>---build_word_dict---- function\n",
            "Load from:  ../data/birds/captions_DAMSM.pickle\n",
            "------------------>>--- {'CONFIG_NAME': 'bird', 'dataset_name': 'birds', 'data_dir': '../data/birds', 'gpu_id': 0, 'imsize': 256, 'z_dim': 100, 'cond_dim': 256, 'manual_seed': 100, 'cuda': True, 'stamp': 'normal', 'state_epoch': 0, 'max_epoch': 2, 'batch_size': 32, 'nf': 32, 'ch_size': 3, 'model': 'base', 'gen_interval': 1, 'test_interval': 10, 'save_interval': 10, 'truncation': True, 'trunc_rate': 0.88, 'sample_times': 10, 'npz_path': '../data/birds/npz/bird_val256_FIDK0.npz', 'example_captions': './eval/example_captions/bird.txt', 'samples_save_dir': './samples/bird/2022_08_12_08_11_09', 'checkpoint': './saved_models/bird/pretrained/state_epoch_1220.pth', 'save_image': False, 'val_save_dir': './vals/bird/', 'TEXT': {'WORDS_NUM': 18, 'EMBEDDING_DIM': 256, 'CAPTIONS_PER_IMAGE': 10, 'DAMSM_NAME': '../data/birds/DAMSMencoder/text_encoder200.pth'}, 'cfg_file': './cfg/bird.yml', 'imgs_per_sent': 16, 'train': False, 'multi_gpus': False, 'local_rank': -1, 'random_sample': True, 'device': device(type='cuda'), 'vocab_size': 5450}\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py:47: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  FutureWarning,\n",
            "Load model weights function is called----\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load model weights function is called----\n",
            "Load model weights function is called----\n",
            "Load ./saved_models/bird/pretrained/state_epoch_1220.pth for NetG\n",
            "************ Start sampling ************\n",
            "==========================>>-----In the---- sample---->>---sample_example---- function\n",
            "****************************************\n",
            "  0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:69: UserWarning: The parameter 'range' is deprecated since 0.12 and will be removed in 0.14. Please use 'value_range' instead.\n",
            "  \"The parameter 'range' is deprecated since 0.12 and will be removed in 0.14. \"\n",
            "100% 2/2 [00:04<00:00,  2.33s/it]\n",
            "****************************************\n",
            "Sampling done, 8.25s cost, saved to ./samples/bird/2022_08_12_08_11_09\n",
            "****************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w48y8E_1atEl",
        "outputId": "afaf5246-a7a0-49ae-a50d-9798dbba8dc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.0+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.50-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting botocore<1.28.0,>=1.27.50\n",
            "  Downloading botocore-1.27.50-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 63.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.50->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.50->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.24.50 botocore-1.27.50 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/train.sh ./cfg/bird.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SgUSPwhJiHD",
        "outputId": "ab509d11-2a6c-49a4-f416-b1475cea4bce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scripts/train.sh: line 12: resume_train: command not found\n",
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use_env is set by default in torchrun.\n",
            "If your script expects `--local_rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  FutureWarning,\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions_DAMSM.pickle\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions_DAMSM.pickle\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py:47: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  FutureWarning,\n",
            "Load model weights function is called----\n",
            "text encoder has called ------>>>>>B_rrn_e\n",
            "100% 407873900/407873900 [00:34<00:00, 11776046.12B/s]\n",
            "in the bert_rnn_encoder\n",
            "bert_rnn_encoder ---->> LSTM\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load model weights function is called----\n",
            "Traceback (most recent call last):\n",
            "  File \"src/train.py\", line 167, in <module>\n",
            "    main(args)\n",
            "  File \"src/train.py\", line 77, in main\n",
            "    image_encoder, text_encoder, netG, netD, netC = prepare_models_new(args)\n",
            "  File \"/content/drive/MyDrive/DF-GAN/code/lib/perpare.py\", line 86, in prepare_models_new\n",
            "    text_encoder = load_model_weights(text_encoder, state_dict, multi_gpus=False)\n",
            "  File \"/content/drive/MyDrive/DF-GAN/code/lib/utils.py\", line 139, in load_model_weights\n",
            "    model.load_state_dict(state_dict)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1605, in load_state_dict\n",
            "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
            "RuntimeError: Error(s) in loading state_dict for BERT_RNN_ENCODER:\n",
            "\tMissing key(s) in state_dict: \"encoder.embeddings.word_embeddings.weight\", \"encoder.embeddings.position_embeddings.weight\", \"encoder.embeddings.token_type_embeddings.weight\", \"encoder.embeddings.LayerNorm.weight\", \"encoder.embeddings.LayerNorm.bias\", \"encoder.encoder.layer.0.attention.self.query.weight\", \"encoder.encoder.layer.0.attention.self.query.bias\", \"encoder.encoder.layer.0.attention.self.key.weight\", \"encoder.encoder.layer.0.attention.self.key.bias\", \"encoder.encoder.layer.0.attention.self.value.weight\", \"encoder.encoder.layer.0.attention.self.value.bias\", \"encoder.encoder.layer.0.attention.output.dense.weight\", \"encoder.encoder.layer.0.attention.output.dense.bias\", \"encoder.encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.0.intermediate.dense.weight\", \"encoder.encoder.layer.0.intermediate.dense.bias\", \"encoder.encoder.layer.0.output.dense.weight\", \"encoder.encoder.layer.0.output.dense.bias\", \"encoder.encoder.layer.0.output.LayerNorm.weight\", \"encoder.encoder.layer.0.output.LayerNorm.bias\", \"encoder.encoder.layer.1.attention.self.query.weight\", \"encoder.encoder.layer.1.attention.self.query.bias\", \"encoder.encoder.layer.1.attention.self.key.weight\", \"encoder.encoder.layer.1.attention.self.key.bias\", \"encoder.encoder.layer.1.attention.self.value.weight\", \"encoder.encoder.layer.1.attention.self.value.bias\", \"encoder.encoder.layer.1.attention.output.dense.weight\", \"encoder.encoder.layer.1.attention.output.dense.bias\", \"encoder.encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.1.intermediate.dense.weight\", \"encoder.encoder.layer.1.intermediate.dense.bias\", \"encoder.encoder.layer.1.output.dense.weight\", \"encoder.encoder.layer.1.output.dense.bias\", \"encoder.encoder.layer.1.output.LayerNorm.weight\", \"encoder.encoder.layer.1.output.LayerNorm.bias\", \"encoder.encoder.layer.2.attention.self.query.weight\", \"encoder.encoder.layer.2.attention.self.query.bias\", \"encoder.encoder.layer.2.attention.self.key.weight\", \"encoder.encoder.layer.2.attention.self.key.bias\", \"encoder.encoder.layer.2.attention.self.value.weight\", \"encoder.encoder.layer.2.attention.self.value.bias\", \"encoder.encoder.layer.2.attention.output.dense.weight\", \"encoder.encoder.layer.2.attention.output.dense.bias\", \"encoder.encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.2.intermediate.dense.weight\", \"encoder.encoder.layer.2.intermediate.dense.bias\", \"encoder.encoder.layer.2.output.dense.weight\", \"encoder.encoder.layer.2.output.dense.bias\", \"encoder.encoder.layer.2.output.LayerNorm.weight\", \"encoder.encoder.layer.2.output.LayerNorm.bias\", \"encoder.encoder.layer.3.attention.self.query.weight\", \"encoder.encoder.layer.3.attention.self.query.bias\", \"encoder.encoder.layer.3.attention.self.key.weight\", \"encoder.encoder.layer.3.attention.self.key.bias\", \"encoder.encoder.layer.3.attention.self.value.weight\", \"encoder.encoder.layer.3.attention.self.value.bias\", \"encoder.encoder.layer.3.attention.output.dense.weight\", \"encoder.encoder.layer.3.attention.output.dense.bias\", \"encoder.encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.3.intermediate.dense.weight\", \"encoder.encoder.layer.3.intermediate.dense.bias\", \"encoder.encoder.layer.3.output.dense.weight\", \"encoder.encoder.layer.3.output.dense.bias\", \"encoder.encoder.layer.3.output.LayerNorm.weight\", \"encoder.encoder.layer.3.output.LayerNorm.bias\", \"encoder.encoder.layer.4.attention.self.query.weight\", \"encoder.encoder.layer.4.attention.self.query.bias\", \"encoder.encoder.layer.4.attention.self.key.weight\", \"encoder.encoder.layer.4.attention.self.key.bias\", \"encoder.encoder.layer.4.attention.self.value.weight\", \"encoder.encoder.layer.4.attention.self.value.bias\", \"encoder.encoder.layer.4.attention.output.dense.weight\", \"encoder.encoder.layer.4.attention.output.dense.bias\", \"encoder.encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.4.intermediate.dense.weight\", \"encoder.encoder.layer.4.intermediate.dense.bias\", \"encoder.encoder.layer.4.output.dense.weight\", \"encoder.encoder.layer.4.output.dense.bias\", \"encoder.encoder.layer.4.output.LayerNorm.weight\", \"encoder.encoder.layer.4.output.LayerNorm.bias\", \"encoder.encoder.layer.5.attention.self.query.weight\", \"encoder.encoder.layer.5.attention.self.query.bias\", \"encoder.encoder.layer.5.attention.self.key.weight\", \"encoder.encoder.layer.5.attention.self.key.bias\", \"encoder.encoder.layer.5.attention.self.value.weight\", \"encoder.encoder.layer.5.attention.self.value.bias\", \"encoder.encoder.layer.5.attention.output.dense.weight\", \"encoder.encoder.layer.5.attention.output.dense.bias\", \"encoder.encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.5.intermediate.dense.weight\", \"encoder.encoder.layer.5.intermediate.dense.bias\", \"encoder.encoder.layer.5.output.dense.weight\", \"encoder.encoder.layer.5.output.dense.bias\", \"encoder.encoder.layer.5.output.LayerNorm.weight\", \"encoder.encoder.layer.5.output.LayerNorm.bias\", \"encoder.encoder.layer.6.attention.self.query.weight\", \"encoder.encoder.layer.6.attention.self.query.bias\", \"encoder.encoder.layer.6.attention.self.key.weight\", \"encoder.encoder.layer.6.attention.self.key.bias\", \"encoder.encoder.layer.6.attention.self.value.weight\", \"encoder.encoder.layer.6.attention.self.value.bias\", \"encoder.encoder.layer.6.attention.output.dense.weight\", \"encoder.encoder.layer.6.attention.output.dense.bias\", \"encoder.encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.6.intermediate.dense.weight\", \"encoder.encoder.layer.6.intermediate.dense.bias\", \"encoder.encoder.layer.6.output.dense.weight\", \"encoder.encoder.layer.6.output.dense.bias\", \"encoder.encoder.layer.6.output.LayerNorm.weight\", \"encoder.encoder.layer.6.output.LayerNorm.bias\", \"encoder.encoder.layer.7.attention.self.query.weight\", \"encoder.encoder.layer.7.attention.self.query.bias\", \"encoder.encoder.layer.7.attention.self.key.weight\", \"encoder.encoder.layer.7.attention.self.key.bias\", \"encoder.encoder.layer.7.attention.self.value.weight\", \"encoder.encoder.layer.7.attention.self.value.bias\", \"encoder.encoder.layer.7.attention.output.dense.weight\", \"encoder.encoder.layer.7.attention.output.dense.bias\", \"encoder.encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.7.intermediate.dense.weight\", \"encoder.encoder.layer.7.intermediate.dense.bias\", \"encoder.encoder.layer.7.output.dense.weight\", \"encoder.encoder.layer.7.output.dense.bias\", \"encoder.encoder.layer.7.output.LayerNorm.weight\", \"encoder.encoder.layer.7.output.LayerNorm.bias\", \"encoder.encoder.layer.8.attention.self.query.weight\", \"encoder.encoder.layer.8.attention.self.query.bias\", \"encoder.encoder.layer.8.attention.self.key.weight\", \"encoder.encoder.layer.8.attention.self.key.bias\", \"encoder.encoder.layer.8.attention.self.value.weight\", \"encoder.encoder.layer.8.attention.self.value.bias\", \"encoder.encoder.layer.8.attention.output.dense.weight\", \"encoder.encoder.layer.8.attention.output.dense.bias\", \"encoder.encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.8.intermediate.dense.weight\", \"encoder.encoder.layer.8.intermediate.dense.bias\", \"encoder.encoder.layer.8.output.dense.weight\", \"encoder.encoder.layer.8.output.dense.bias\", \"encoder.encoder.layer.8.output.LayerNorm.weight\", \"encoder.encoder.layer.8.output.LayerNorm.bias\", \"encoder.encoder.layer.9.attention.self.query.weight\", \"encoder.encoder.layer.9.attention.self.query.bias\", \"encoder.encoder.layer.9.attention.self.key.weight\", \"encoder.encoder.layer.9.attention.self.key.bias\", \"encoder.encoder.layer.9.attention.self.value.weight\", \"encoder.encoder.layer.9.attention.self.value.bias\", \"encoder.encoder.layer.9.attention.output.dense.weight\", \"encoder.encoder.layer.9.attention.output.dense.bias\", \"encoder.encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.9.intermediate.dense.weight\", \"encoder.encoder.layer.9.intermediate.dense.bias\", \"encoder.encoder.layer.9.output.dense.weight\", \"encoder.encoder.layer.9.output.dense.bias\", \"encoder.encoder.layer.9.output.LayerNorm.weight\", \"encoder.encoder.layer.9.output.LayerNorm.bias\", \"encoder.encoder.layer.10.attention.self.query.weight\", \"encoder.encoder.layer.10.attention.self.query.bias\", \"encoder.encoder.layer.10.attention.self.key.weight\", \"encoder.encoder.layer.10.attention.self.key.bias\", \"encoder.encoder.layer.10.attention.self.value.weight\", \"encoder.encoder.layer.10.attention.self.value.bias\", \"encoder.encoder.layer.10.attention.output.dense.weight\", \"encoder.encoder.layer.10.attention.output.dense.bias\", \"encoder.encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.10.intermediate.dense.weight\", \"encoder.encoder.layer.10.intermediate.dense.bias\", \"encoder.encoder.layer.10.output.dense.weight\", \"encoder.encoder.layer.10.output.dense.bias\", \"encoder.encoder.layer.10.output.LayerNorm.weight\", \"encoder.encoder.layer.10.output.LayerNorm.bias\", \"encoder.encoder.layer.11.attention.self.query.weight\", \"encoder.encoder.layer.11.attention.self.query.bias\", \"encoder.encoder.layer.11.attention.self.key.weight\", \"encoder.encoder.layer.11.attention.self.key.bias\", \"encoder.encoder.layer.11.attention.self.value.weight\", \"encoder.encoder.layer.11.attention.self.value.bias\", \"encoder.encoder.layer.11.attention.output.dense.weight\", \"encoder.encoder.layer.11.attention.output.dense.bias\", \"encoder.encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.11.intermediate.dense.weight\", \"encoder.encoder.layer.11.intermediate.dense.bias\", \"encoder.encoder.layer.11.output.dense.weight\", \"encoder.encoder.layer.11.output.dense.bias\", \"encoder.encoder.layer.11.output.LayerNorm.weight\", \"encoder.encoder.layer.11.output.LayerNorm.bias\", \"encoder.pooler.dense.weight\", \"encoder.pooler.dense.bias\", \"bert_linear.weight\", \"bert_linear.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"encoder.weight\". \n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 515) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 193, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 189, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 174, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py\", line 755, in run\n",
            "    )(*cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 247, in launch_agent\n",
            "    failures=result.failures,\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "src/train.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2022-08-12_08:13:01\n",
            "  host      : 6c22ce2de9a3\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 515)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nDvfr3e1jjH7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}